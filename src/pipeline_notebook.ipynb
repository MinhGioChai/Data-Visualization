{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fef5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e8cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../raw_data/full_data.csv')  # original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f318b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_TYPE_SUITE\n",
       "Unaccompanied      173985\n",
       "Family              28107\n",
       "Spouse, partner      7983\n",
       "Children             2312\n",
       "Other_B              1190\n",
       "NaN                   882\n",
       "Other_A               605\n",
       "Group of people       194\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NAME_TYPE_SUITE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f389747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 47 house columns with more than threshold% missing values\n"
     ]
    }
   ],
   "source": [
    "df_prep = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d053a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 NAME_TYPE_SUITE - missing 882 values(MAR))\n",
    "df_prep['NAME_TYPE_SUITE'] = df_prep['NAME_TYPE_SUITE'].fillna('Unknown') \n",
    "df_prep['NAME_TYPE_SUITE'] = df_prep['NAME_TYPE_SUITE'].astype('category')\n",
    "\n",
    "# 2. AMT_GOODS_PRICE – missing(MCAR) 174 values\n",
    "df_prep['GOODS_PRICE_WAS_MISSING'] = df_prep['AMT_GOODS_PRICE'].isnull().astype(float)\n",
    "df_prep['AMT_GOODS_PRICE'] = df_prep['AMT_GOODS_PRICE'].fillna(df_prep['AMT_CREDIT'])\n",
    "\n",
    "# 3. AMT_ANNUITY – only 9 missing values(MCAR) → fill in the median to avoid lossing of important feature data\n",
    "df_prep['AMT_ANNUITY_WAS_MISSING'] = df_prep['AMT_ANNUITY'].isnull().astype(float)\n",
    "df_prep['AMT_ANNUITY'] = df_prep['AMT_ANNUITY'].fillna(df_prep['AMT_ANNUITY'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4a37ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OCCUPATION_TYPE'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b022af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 215,258 rows\n"
     ]
    }
   ],
   "source": [
    "# XỬ LÝ nan CODE_GENDER\n",
    "print(f\"Original dataset size: {len(df_prep):,} rows\")\n",
    "xna_gender_count = (df_prep['CODE_GENDER'] == 'XNA').sum()\n",
    "# print(f\"Rows with CODE_GENDER == 'XNA': {xna_gender_count:,} (will be dropped)\")\n",
    "# df_prep = df_prep[df_prep['CODE_GENDER'] != 'XNA']  \n",
    "\n",
    "# XỬ LÝ nan cho OWN_CAR_AGE\n",
    "\n",
    "non_owners_missing = ((df_prep['FLAG_OWN_CAR'] == 'N') & df_prep['OWN_CAR_AGE'].isna()).sum()\n",
    "owners_missing = ((df_prep['FLAG_OWN_CAR'] == 'Y') & df_prep['OWN_CAR_AGE'].isna()).sum()\n",
    "\n",
    "# Fill 0 cho bọn không có xe với độ tuổi xe bị thiếu\n",
    "df_prep.loc[(df_prep['FLAG_OWN_CAR'] == 'N') & df_prep['OWN_CAR_AGE'].isna(), 'OWN_CAR_AGE'] = 0\n",
    "median_car_age = df_prep.loc[df_prep['FLAG_OWN_CAR'] == 'Y', 'OWN_CAR_AGE'].median()\n",
    "\n",
    "df_prep.loc[\n",
    "    (df_prep['FLAG_OWN_CAR'] == 'Y') & (df_prep['OWN_CAR_AGE'].isna()),\n",
    "    'OWN_CAR_AGE'\n",
    "] = median_car_age \n",
    "\n",
    "# XỬ LÝ DAYS_EMPLOYED: \n",
    "df_prep['DAYS_EMPLOYED'] = df_prep['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "df_prep['DAYS_EMPLOYED'] = df_prep['DAYS_EMPLOYED'].fillna(0)\n",
    "\n",
    "# XỬ LÝ OCCUPATION_TYPE\n",
    "def impute_occupation(df):\n",
    "    occ = df['OCCUPATION_TYPE'].copy()\n",
    "    missing_mask = occ.isna() | (occ == '')\n",
    "    # Conditions indicating likely unemployment / no defined occupation\n",
    "    income_unemp = df['NAME_INCOME_TYPE'].str.contains('Pensioner|Unemployed', case=False, regex=True, na=False)\n",
    "    no_employment_duration = df['DAYS_EMPLOYED'].isna() | (df['DAYS_EMPLOYED'] >= 0)  # người missing employment duration hoặc không có ngày làm việc hợp lệ\n",
    "    # Cho thành 'Unemployed' nơi cả occupation missing và đang trong độ tuổi không có nghề nghiệp\n",
    "    occ.loc[missing_mask & (income_unemp | no_employment_duration)] = 'Unemployed'\n",
    "    # Những cái còn lại gán thành 'Laborers'\n",
    "    occ.loc[missing_mask & ~(income_unemp | no_employment_duration)] = 'Laborers'\n",
    "    return occ\n",
    "\n",
    "df_prep['OCCUPATION_TYPE'] = impute_occupation(df_prep)\n",
    "\n",
    "# XỬ LÝ CNT_FAM_MEMBERS và DAYS_LAST_PHONE_CHANGE\n",
    "# 3. CNT_FAM_MEMBERS median imputation \n",
    "df_prep['CNT_FAM_MEMBERS'] = df_prep['CNT_FAM_MEMBERS'].fillna(df_prep['CNT_FAM_MEMBERS'].median())\n",
    "\n",
    "# 4. DAYS_LAST_PHONE_CHANGE median imputation\n",
    "df_prep['DAYS_LAST_PHONE_CHANGE'] = df_prep['DAYS_LAST_PHONE_CHANGE'].fillna(df_prep['DAYS_LAST_PHONE_CHANGE'].median())\n",
    "\n",
    "# fillna ORGANIZATION_TYPE with mode\n",
    "df_prep['ORGANISATION_TYPE'] = df['ORGANIZATION_TYPE'].fillna(df['ORGANIZATION_TYPE'].mode()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be31400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep['AGE'] = (-df_prep['DAYS_BIRTH'] / 365.25).astype(int)\n",
    "df_prep.drop(columns=['DAYS_BIRTH'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4316856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# process ext features\n",
    "ext_cols = [\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]\n",
    "\n",
    "df_ext = df[ext_cols].copy()\n",
    "\n",
    "# BƯỚC XỬ LÍ MISSING\n",
    "\n",
    "# 1. Dùng Simpleimputer( mean/median/0)\n",
    "#mean/median\n",
    "def simple_impute(df, strategy=\"mean\"):\n",
    "    imp = SimpleImputer(strategy=strategy)\n",
    "    arr = imp.fit_transform(df)\n",
    "    return pd.DataFrame(arr, columns=[c + f\"_simple_{strategy}\" for c in df.columns])\n",
    "\n",
    "# df_ext_mean = simple_impute(df_ext, strategy=\"mean\")\n",
    "# df_ext_median = simple_impute(df_ext, strategy=\"median\")\n",
    "\n",
    "# constant = 0\n",
    "def simple_impute_zero(df):\n",
    "    imp = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "    arr = imp.fit_transform(df)\n",
    "    return pd.DataFrame(arr, columns=[c + \"_simple_0\" for c in df.columns])\n",
    "\n",
    "# df_ext_zero = simple_impute_zero(df_ext)\n",
    "\n",
    "# 2. Dùng KNN\n",
    "def impute_ext_knn(df, k=5, top_n=5):\n",
    "    \"\"\"\n",
    "    Impute EXT_SOURCE_1/2/3 using KNNImputer.\n",
    "    - Selects top_n features with highest absolute correlation\n",
    "      with the EXT_SOURCE columns.\n",
    "    - Uses ONLY those features + EXT columns to fit KNN.\n",
    "    Returns a DataFrame of the 3 imputed EXT_SOURCE_* columns.\n",
    "    \"\"\"\n",
    "\n",
    "    ext_cols = [\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]\n",
    "    ext = df[ext_cols].copy()\n",
    "\n",
    "    # ==== 1. Compute correlations ====\n",
    "    corr = df.corr(numeric_only=True)[ext_cols]\n",
    "\n",
    "    # ==== 2. Get top_n most correlated features per EXT column ====\n",
    "    top_features = set()  # use set to avoid duplicates\n",
    "    for col in ext_cols:\n",
    "        # drop the EXT_SOURCE columns to avoid trivial self-correlation\n",
    "        top_corr = corr[col].drop(ext_cols, errors=\"ignore\")\n",
    "        # pick highest absolute correlations\n",
    "        top_n_feats = top_corr.abs().nlargest(top_n).index.tolist()\n",
    "        top_features.update(top_n_feats)\n",
    "\n",
    "    top_features = list(top_features)\n",
    "    print(f'The selected top correlated features for KNN Imputer: {top_features}')\n",
    "\n",
    "    # ==== 3. Build KNN input data ====\n",
    "    # Use EXT_SOURCE columns + selected correlated features\n",
    "    knn_input_cols = ext_cols + top_features\n",
    "    knn_df = df[knn_input_cols].copy()\n",
    "\n",
    "    # ==== 4. Apply KNN Imputer ====\n",
    "    knn = KNNImputer(n_neighbors=k)\n",
    "    knn_arr = knn.fit_transform(knn_df)\n",
    "\n",
    "    knn_df_imputed = pd.DataFrame(knn_arr, columns=knn_input_cols, index=df.index)\n",
    "\n",
    "    # ==== 5. Return only the imputed EXT columns ====\n",
    "    return knn_df_imputed[ext_cols]\n",
    "\n",
    "\n",
    "# df_ext_knn =impute_ext_knn(df_ext)\n",
    "\n",
    "# BƯỚC GỘP THÀNH 1 FEATURE\n",
    "#  1. Mean score \n",
    "def combine_mean(df):\n",
    "    df_out = df.copy()\n",
    "    df_out[\"EXT_SOURCE_MEAN\"] = df.mean(axis=1)\n",
    "    return df_out[[\"EXT_SOURCE_MEAN\"]]\n",
    "\n",
    "#  2. PCA 1 component \n",
    "def combine_pca(df):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    comp = pca.fit_transform(X_scaled)\n",
    "\n",
    "    return pd.DataFrame(comp, columns=[\"EXT_SOURCE_PCA1\"])\n",
    "\n",
    "#  3. weighted average \n",
    "def combine_weight(df, w1=0.2, w2=0.5, w3=0.3):\n",
    "    weights = np.array([w1, w2, w3])\n",
    "    new_col = df.values.dot(weights)\n",
    "    return pd.DataFrame(new_col, columns=[\"EXT_SOURCE_WEIGHTED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a705cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_social = [\n",
    "    \"OBS_30_CNT_SOCIAL_CIRCLE\", \"OBS_60_CNT_SOCIAL_CIRCLE\",\n",
    "    \"DEF_30_CNT_SOCIAL_CIRCLE\", \"DEF_60_CNT_SOCIAL_CIRCLE\"\n",
    "]\n",
    "\n",
    "df_social = df[cols_social].copy()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "304fe6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treat ext columns and social columns by simple imputation\n",
    "df_prep[ext_cols] = simple_impute(df_ext, strategy=\"median\")\n",
    "df_prep[cols_social] = simple_impute(df_social, strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b392e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5968b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_cols = [f'FLAG_DOCUMENT_{i}' for i in range(2, 22)]\n",
    "credit_bureau_cols = [\n",
    "    'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "    'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "    'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "    'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "    'AMT_REQ_CREDIT_BUREAU_YEAR'\n",
    "    ]\n",
    "# Feature 1: Binary flag for missingness\n",
    "df_prep['HAS_CREDIT_BUREAU_DATA'] = (~df_prep['AMT_REQ_CREDIT_BUREAU_HOUR'].isna()).astype(int)\n",
    "        \n",
    "# Feature 2: Fill 0 for all bureau counts\n",
    "df_prep[credit_bureau_cols] = df_prep[credit_bureau_cols].fillna(0) \n",
    "\n",
    "\n",
    "df_prep['TOTAL_DOC_SUBMITTED'] = df[document_cols].sum(axis=1)\n",
    "drop_cols = [col for col in df_prep.columns if col.startswith('FLAG_DOCUMENT_')]\n",
    "df_prep = df_prep.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d06c7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_EDUCATION_TYPE\n",
       "Secondary / secondary special    152933\n",
       "Higher education                  52389\n",
       "Incomplete higher                  7164\n",
       "Lower secondary                    2656\n",
       "Academic degree                     116\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NAME_EDUCATION_TYPE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85e99859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CODE_GENDER\n",
       "F      141571\n",
       "M       73683\n",
       "XNA         4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep['CODE_GENDER'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bureau_categorical(df, credit_bureau_cols):\n",
    "    \n",
    "    for col in credit_bureau_cols:\n",
    "        new_col = col + '_CAT'\n",
    "        \n",
    "        # Define bins dựa trên distribution\n",
    "        if col in ['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY']:\n",
    "            # HOUR/DAY: 0 vs >0\n",
    "            df[new_col] = (df[col] > 0).astype(int)\n",
    "            df[new_col] = df[new_col].map({0: 'ZERO', 1: 'HAS_ENQUIRY'})\n",
    "            \n",
    "        elif col in ['AMT_REQ_CREDIT_BUREAU_WEEK']:\n",
    "            # WEEK: 0, 1, >1\n",
    "            df[new_col] = 'ZERO'\n",
    "            df.loc[df[col] == 1, new_col] = 'ONE'\n",
    "            df.loc[df[col] > 1, new_col] = 'MULTIPLE'\n",
    "            \n",
    "        elif col in ['AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT']:\n",
    "            # MONTH/QRT: 0, 1-2, >2\n",
    "            df[new_col] = 'ZERO'\n",
    "            df.loc[df[col].between(1, 2), new_col] = 'LOW'\n",
    "            df.loc[df[col] > 2, new_col] = 'HIGH'\n",
    "            \n",
    "        else:  # YEAR\n",
    "            # YEAR: 0, 1-2, 3-5, >5\n",
    "            df[new_col] = 'ZERO'\n",
    "            df.loc[df[col].between(1, 2), new_col] = 'LOW'\n",
    "            df.loc[df[col].between(3, 5), new_col] = 'MEDIUM'\n",
    "            df.loc[df[col] > 5, new_col] = 'HIGH'\n",
    "\n",
    "        df = df.drop(col, axis=1)\n",
    "    \n",
    "    return df               # clip: loại bỏ những giá trị cực đoan\n",
    "df_prep = encode_bureau_categorical(df_prep, credit_bureau_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "683b0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change object dtype to category dtype\n",
    "for col in df_prep.select_dtypes(include='object').columns:\n",
    "    df_prep[col] = df_prep[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c334f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OCCUPATION_TYPE\n",
       "Laborers                 67524\n",
       "Unemployed               38796\n",
       "Sales staff              22337\n",
       "Core staff               19349\n",
       "Managers                 14899\n",
       "Drivers                  12997\n",
       "High skill tech staff     7900\n",
       "Accountants               6914\n",
       "Medicine staff            6002\n",
       "Security staff            4738\n",
       "Cooking staff             4184\n",
       "Cleaning staff            3215\n",
       "Private service staff     1838\n",
       "Low-skill Laborers        1476\n",
       "Waiters/barmen staff       917\n",
       "Secretaries                914\n",
       "Realty agents              505\n",
       "HR staff                   392\n",
       "IT staff                   361\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        occ = X['OCCUPATION_TYPE'].copy()\n",
    "        missing_mask = occ.isna() | (occ == '')\n",
    "        \n",
    "        # Conditions indicating likely unemployment\n",
    "        income_unemp = X['NAME_INCOME_TYPE'].str.contains(\n",
    "            'Pensioner|Unemployed', case=False, regex=True, na=False\n",
    "        )\n",
    "        no_employment_duration = X['DAYS_EMPLOYED'].isna() | (X['DAYS_EMPLOYED'] >= 0)\n",
    "        \n",
    "        # Assign 'Unemployed' where appropriate\n",
    "        occ.loc[missing_mask & (income_unemp | no_employment_duration)] = 'Unemployed'\n",
    "        \n",
    "        # Assign remaining missing as 'Laborers'\n",
    "        occ.loc[missing_mask & ~(income_unemp | no_employment_duration)] = 'Laborers'\n",
    "        \n",
    "        X['OCCUPATION_TYPE'] = occ\n",
    "        return X\n",
    "df_ = transform(df)\n",
    "df_['OCCUPATION_TYPE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf08fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. HÀM CHECK & CAP OUTLIER BẰNG IQR\n",
    "def cap_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    return series.clip(lower, upper)\n",
    "def cap_percentile(series, p=0.99):\n",
    "    upper = series.quantile(p)\n",
    "    return series.clip(upper=upper)\n",
    "def process_outlier(df_prep):\n",
    "    for col in cols_social:\n",
    "        df_prep[col + \"_capped_iqr\"] = cap_iqr(df_prep[col])\n",
    "\n",
    "    for col in cols_social:\n",
    "        df_prep[col + \"_capped_p99\"] = cap_percentile(df_prep[col])\n",
    "    \n",
    "    # 1. Dùng threshold \n",
    "    caps = {\n",
    "        'AMT_INCOME_TOTAL' : np.percentile(df_prep['AMT_INCOME_TOTAL'], 99.5),   \n",
    "        'AMT_CREDIT'       : np.percentile(df_prep['AMT_CREDIT'], 99.5),        \n",
    "        'AMT_ANNUITY'      : np.percentile(df_prep['AMT_ANNUITY'], 99.5),       \n",
    "        'AMT_GOODS_PRICE'  : np.percentile(df_prep['AMT_GOODS_PRICE'], 99.5),   \n",
    "    }\n",
    "\n",
    "    # 2. Winsorize (cắt ngọn)(mọi giá trị lớn hơn threshold bị ép xuống bằng threshold) + tạo flag outlier \n",
    "    for col, threshold in caps.items():\n",
    "        df_prep[f'{col}_outlier'] = (df_prep[col] > threshold).astype(int)   # flag: giữ lại thông tin nguwofi này là từng là cực giàu/ vay cực lớn\n",
    "        df_prep[col] = df_prep[col].clip(upper=threshold) \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba8eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
